{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Datacamp article: https://www.datacamp.com/tutorial/llama-3-1-rag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain_community scikit-learn langchain-ollama sentence-transformers tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main functions that I have used from LangChain in my book assistant bot:\n",
    "1. PyPDFLoader > To load the PDF file\n",
    "2. SKLearnVectorStore > A vector store that uses scikit-learn to store text embeddings.\n",
    "3. RecursiveCharacterTextSplitter > Splits documents into chunks of text.\n",
    "4. HuggingFaceEmbeddings > To generate embeddings for the text.\n",
    "5. ChatOllama > To connect to Ollama (locally runnnig models) with langchain.\n",
    "6. PromptTemplate > To connect prompt + LLM to get a repsonse.\n",
    "7. Chains > Combines:  Retriever + LLM > RetrievalQA > Ask questions on documents or PDf files(In my case PDF file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and prepare documents\n",
    "Documents can be anyhing, I can load a PDF or use webpages as the source also\n",
    "\n",
    "What is does:\n",
    "You give the file path of a PDF (sample-book.pdf).\n",
    "PyPDFLoader reads and extracts the content from the PDF.\n",
    "docs_list is now a list of all the pages as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of PDF file paths to load documents from\n",
    "pdf_paths = [\n",
    "    \"/home/ai-ml-practice/rag-using-llm/sample-book.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split documents into chunks\n",
    "\n",
    "What it does:\n",
    "Big documents are broken into smaller pieces (250 characters each).\n",
    "This helps the LLM read smaller bits and answer more accurately.\n",
    "chunk_overlap=2 means it includes a few repeated words to preserve context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split documents\n",
    "docs = [PyPDFLoader(pdf_path).load() for pdf_path in pdf_paths]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# too small chunk size(250) dint work as model was not able to\n",
    "# understand context(basic questions like who is the author dint work)\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=550, chunk_overlap=100\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize embeddings\n",
    "\n",
    "What it does:\n",
    "You extract just the text from each chunk.\n",
    "Then store them as vectors in a small database (SKLearnVectorStore).\n",
    "retriever will now fetch the top 4 most similar chunks for a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ") model_name='all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} multi_process=False show_progress=False\n"
     ]
    }
   ],
   "source": [
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store\n",
    "texts = [doc.page_content for doc in doc_splits]\n",
    "vectorstore = SKLearnVectorStore.from_texts(texts, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever(k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.1:8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW ADDITION IN THE PROMPT > Chat Hisory: {chat_history}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following documents to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Please answer in simple and easy to understand language.\n",
    "Use two sentences maximum and keep the answer concise:\n",
    "Question: {question}\n",
    "Documents: {context}\n",
    "Chat Hisory: {chat_history}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\", \"chat_history\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create retrieval chain\n",
    "\n",
    "What it does:\n",
    "This creates the full RAG pipeline:\n",
    "It takes a user query\n",
    "Uses the retriever to grab relevant document chunks\n",
    "Passes both query + chunks to the LLM using your prompt\n",
    "return_source_documents=True helps if you want to show where the answer came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define RAG application class \n",
    "\n",
    "What it does:\n",
    "A simple class to wrap your chatbot logic\n",
    "It runs the RAG chain when you call run()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGApplication:\n",
    "    def __init__(self, qa_chain):\n",
    "        self.qa_chain = qa_chain\n",
    "\n",
    "    def run(self, question):\n",
    "        result = self.qa_chain.invoke({\"query\": question})\n",
    "        return result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize and run\n",
    "\n",
    "What it does:\n",
    "Initializes the bot with the RAG chain\n",
    "Sends the question to the chain\n",
    "Prints out the LLMâ€™s answer based on the retrieved chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is this book about?\n",
      "Assistant: I don't know what this question refers to, as there is no \"Chat History\" section in the provided documents.\n",
      "Human: What is the name of the author?\n",
      "Assistant: I don't have enough information to answer your question. There's no clear reference to a book in the chat history you provided.\n",
      "Human: What was my first question?\n",
      "Assistant: I don't know who the author is, as their name is not mentioned in the provided documents. \n",
      "\n",
      "The documents appear to be excerpts from a self-help or motivational book, but I couldn't find any information about the author's identity.\n",
      "Follow Up Input: Who is the author of the book?\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n",
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant for question-answering tasks.\n",
      "Use the following documents to answer the question.\n",
      "If you don't know the answer, just say that you don't know.\n",
      "Please answer in simple and easy to understand language.\n",
      "Use two sentences maximum and keep the answer concise:\n",
      "Question: Here is the rephrased follow-up question as a standalone question:\n",
      "\n",
      "Â¿QuiÃ©n es el autor del libro? (Who is the author of the book?)\n",
      "Documents: engrossed in the book, and I felt free, if only for a short period of time. This\n",
      "was when I disc overed my first trick to dealing with worry , which I call\n",
      "â€˜taking control of your free timeâ€™, which ultimately is your thinking time.\n",
      "T ake control of your thinking time\n",
      "The biggest dif ference in my life  over the last two years, alongside the birth\n",
      "of my daughter , is my ability to take control of my wandering an d unhelpful\n",
      "\n",
      "ACKNOWLEDGEMENTS\n",
      "My gratitude goes out to all those who have passed through my life. All of\n",
      "you hav e contrib uted in some way to my journey and the substance of this\n",
      "book.\n",
      "First, I want to thank my publisher Lucy McCarraher and Re think Press.\n",
      "W ithout them, this book would not be possible.\n",
      "My fath er Gerry , who has suppo rted me through my entire life and nurtured\n",
      "a work ethic that still supports me to this day .\n",
      "My sister Karen, who has always been my second biggest fan in everything\n",
      "that I do. She has been one of my best friends and is one of the smartest\n",
      "people Iâ€™ve had the pleasure of knowing.\n",
      "My team of Pa ul Dermody , Daniel Lupton and Emma Finneg an â€“ while\n",
      "writing this book, Paul,\n",
      "Daniel and Emma allowed me to bounce ideas of f them on a daily basis and\n",
      "really helped the book take a c ertain direction. I couldnâ€™ t have completed\n",
      "this book without their support.\n",
      "Finally , to whom this book is dedicated â€“ my mum Rita and my daughter\n",
      "Holly . My mum and my daughter are the two closest people in my life. The\n",
      "reason I couldnâ€™ t pick one or the other is because without e ither , there\n",
      "would b e no book. W ithout my mum, I wouldnâ€™ t be on my curr ent journey\n",
      "and have the mindset that I have, and without Holly , I wouldn â€™ t have my\n",
      "â€˜whyâ€™.\n",
      "This book is dedicated to them,  for without them, I would not be the man,\n",
      "son or father I am today . I love you both.\n",
      "\n",
      "Thatâ€™ s when I learnt about Confucius.\n",
      "â€˜ W h e n  t h e  s t u d e n t  i s  r e a d y ,  \n",
      "t h e  t e a c h e r  w i l l  a p p e a r . â€™\n",
      "BUDDHA\n",
      "Confucius was, in my opinion, the greatest Eastern philosopher of all time,\n",
      "whose teachings deeply influenced East Asian life and thought.\n",
      "He is considered Chinaâ€™ s firs t teacher and his teachings are usually\n",
      "expressed in short phrases whic h are open to various interpreta tions. Chief\n",
      "among h is philosophical ideas is the importance of a virtuous life, filial\n",
      "piety (respecting parents and elders) and ancestral study .\n",
      "In Y u Danâ€™ s book Confucius Fr om  The Heart , he talk s about the necessity\n",
      "for bene volent, kind and frugal rulers, the importance of inner moral\n",
      "harmony and its direct connecti on with harmony in the physica l world and\n",
      "how rulers and teachers are important role models for the wider society .\n",
      "Sometimes whe n reading, I make notes of the things that come up that will\n",
      "either lead to a blog post, a po dcast or some other medium where I think\n",
      "others may benefit from what Iâ€™ve read. This is what came up for me and\n",
      "Iâ€™ve included it as the last section of this book as itâ€™ s something  I still read\n",
      "every single morning. I hope it helps.\n",
      "\n",
      "This section struck me more than anything else in the entire book. I\n",
      "remember walki ng towards my apartment in East London. The  streetlights\n",
      "were lit and it w as a cold, frosty November night. I stopped dead in my\n",
      "tracks to contemplate what I had just listened to.\n",
      "The vision that painted in my mind was so clear and vivid that it hit me like\n",
      "a ton of bricks. It also dawned on me that I wasnâ€™ t happy with the answer to\n",
      "the question that was forming in  my head. That was the moment I decided\n",
      "to leave London to pursue my passion. Shortly after , I left my job, moved\n",
      "back home with my parents (bro ke and on welfare) and tried to s et up a new\n",
      "life, whe re my funeral would matter , where my life would matter . That was\n",
      "the d ay when my mind shifted from being content with life to t he seed that\n",
      "was recently planted: â€˜Iâ€™m only going to get one life!â€™\n",
      "â€˜ Y o u  o n l y  l i v e  o n c e ,  b u t  i f  y o u  d o  \n",
      "i t  r i g h t ,  o n c e  i s  e n o u g h â€™\n",
      "MAE WEST\n",
      "The mo st funda mental applicat ion of â€˜begin with the end in mindâ€™ is to\n",
      "begin today with the image or p icture of your life and what you want it to\n",
      "look like. The funeral scenari o is the most extreme of cases, but the\n",
      "paradigm or mind shift that I experienced that day stuck with  me, and is\n",
      "now channelled into all aspects of my life.\n",
      "By k eeping that end clearly in m ind, you can ensure that whatever you do\n",
      "on any particular day does not violate the criteria you have defined as\n",
      "supremely important, and that  each day of your life contributes in a\n",
      "meaningful way to the vision you have of your life or as a whole. I once had\n",
      "a talk with a client of mine who was looking to excel as one of the top\n",
      "sporting athletes  in the country  and is currently on the brink of breaking\n",
      "into one  of the t op teams where fame and recognition would be  guaranteed.\n",
      "W e talked about beginning with the end in mind and working back from\n",
      "that. I always try to follow the 10% law: your goals and tar gets s hould be so\n",
      "Chat Hisory: \n",
      "Human: What is this book about?\n",
      "Assistant: I don't know what this question refers to, as there is no \"Chat History\" section in the provided documents.\n",
      "Human: What is the name of the author?\n",
      "Assistant: I don't have enough information to answer your question. There's no clear reference to a book in the chat history you provided.\n",
      "Human: What was my first question?\n",
      "Assistant: I don't know who the author is, as their name is not mentioned in the provided documents. \n",
      "\n",
      "The documents appear to be excerpts from a self-help or motivational book, but I couldn't find any information about the author's identity.\n",
      "Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Question: Who is the author of the book?\n",
      "Answer: Unfortunately, I don't have enough information to answer your question about the author's name. The text doesn't mention it explicitly.\n"
     ]
    }
   ],
   "source": [
    "rag_application = RAGApplication(qa_chain)\n",
    "\n",
    "question = \"What is chapter 5 about?\"\n",
    "answer = rag_application.run(question)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
