{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Datacamp article: https://www.datacamp.com/tutorial/llama-3-1-rag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain_community scikit-learn langchain-ollama sentence-transformers tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main functions that I have used from LangChain in my book assistant bot:\n",
    "1. PyPDFLoader > To load the PDF file\n",
    "2. SKLearnVectorStore > A vector store that uses scikit-learn to store text embeddings.\n",
    "3. RecursiveCharacterTextSplitter > Splits documents into chunks of text.\n",
    "4. HuggingFaceEmbeddings > To generate embeddings for the text.\n",
    "5. ChatOllama > To connect to Ollama (locally runnnig models) with langchain.\n",
    "6. PromptTemplate > To connect prompt + LLM to get a repsonse.\n",
    "7. Chains > Combines:  Retriever + LLM > RetrievalQA > Ask questions on documents or PDf files(In my case PDF file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and prepare documents\n",
    "Documents can be anyhing, I can load a PDF or use webpages as the source also\n",
    "\n",
    "What is does:\n",
    "You give the file path of a PDF (sample-book.pdf).\n",
    "PyPDFLoader reads and extracts the content from the PDF.\n",
    "docs_list is now a list of all the pages as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of PDF file paths to load documents from\n",
    "pdf_paths = [\n",
    "    \"/home/ai-ml-practice/rag-using-llm/sample-book.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split documents into chunks\n",
    "\n",
    "What it does:\n",
    "Big documents are broken into smaller pieces (250 characters each).\n",
    "This helps the LLM read smaller bits and answer more accurately.\n",
    "chunk_overlap=2 means it includes a few repeated words to preserve context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split documents\n",
    "docs = [PyPDFLoader(pdf_path).load() for pdf_path in pdf_paths]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=2\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize embeddings\n",
    "\n",
    "What it does:\n",
    "You extract just the text from each chunk.\n",
    "Then store them as vectors in a small database (SKLearnVectorStore).\n",
    "retriever will now fetch the top 4 most similar chunks for a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store\n",
    "texts = [doc.page_content for doc in doc_splits]\n",
    "vectorstore = SKLearnVectorStore.from_texts(texts, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever(k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.1:8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following documents to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Please answer in simple and easy to understand language.\n",
    "Use two sentences maximum and keep the answer concise:\n",
    "Question: {question}\n",
    "Documents: {context}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create retrieval chain\n",
    "\n",
    "What it does:\n",
    "This creates the full RAG pipeline:\n",
    "It takes a user query\n",
    "Uses the retriever to grab relevant document chunks\n",
    "Passes both query + chunks to the LLM using your prompt\n",
    "return_source_documents=True helps if you want to show where the answer came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define RAG application class \n",
    "\n",
    "What it does:\n",
    "A simple class to wrap your chatbot logic\n",
    "It runs the RAG chain when you call run()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGApplication:\n",
    "    def __init__(self, qa_chain):\n",
    "        self.qa_chain = qa_chain\n",
    "\n",
    "    def run(self, question):\n",
    "        result = self.qa_chain.invoke({\"query\": question})\n",
    "        return result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize and run\n",
    "\n",
    "What it does:\n",
    "Initializes the bot with the RAG chain\n",
    "Sends the question to the chain\n",
    "Prints out the LLMâ€™s answer based on the retrieved chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What should I know about protein?\n",
      "Answer: Here's the answer in two sentences:\n",
      "\n",
      "Protein is important for growth, tissue repair, immune function, energy production, and preserving lean muscle mass. You can get protein from animal products like meat, eggs, and fish, or plant-based sources like beans, nuts, and grains, as long as you combine them to get all essential amino acids.\n"
     ]
    }
   ],
   "source": [
    "rag_application = RAGApplication(qa_chain)\n",
    "\n",
    "question = \"What should I know about protein?\"\n",
    "answer = rag_application.run(question)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
