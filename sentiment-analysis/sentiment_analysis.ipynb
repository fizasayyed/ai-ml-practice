{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gensim\n",
    "# % pip install pandas==1.5.3\n",
    "# % pip install scikit-learn==1.0.2\n",
    "# % pip install gensim==4.1.2\n",
    "# % pip install nltk==3.6.7\n",
    "# % pip install numpy==1.21.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split    # for 2 split - train test\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # For bag of words\n",
    "from sklearn.naive_bayes import MultinomialNB  # Naive bayes models\n",
    "from sklearn.model_selection import GridSearchCV        # For Hyperparameters(finding best params)\n",
    "from sklearn.metrics import accuracy_score, classification_report # for evaluation\n",
    "import re # regualr expression for removing punctuations and stuff\n",
    "from nltk.corpus import stopwords       # for stopwords removal\n",
    "from nltk.stem import WordNetLemmatizer # For lemmatization\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer  # For TF-IDF vectoriser\n",
    "# from nltk.stem import PorterStemmer     # For stemming\n",
    "# from gensim.models import Word2Vec      # For Word2vec neural network based vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Load the data (In my case CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               text     label\n",
      "0             Enjoying a beautiful day at the park!  positive\n",
      "1                Just finished an amazing workout!   positive\n",
      "2       Excited about the upcoming weekend getaway!  positive\n",
      "3   Feeling grateful for the little things in life.  positive\n",
      "4  Rainy days call for cozy blankets and hot cocoa.  positive\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/GitHub/text_summarisation_trad_NLP/sentiments.csv\")\n",
    "\n",
    "# Should print : 'text', 'label' and the coulmns associated to them (Uncleaned data)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Data Cleaning > Lowercasing, punctuation and number removal, stopwords removal, lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stop words -> we will use stopwords removal which would remove words like, the of\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize and remove stop words\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning function to the text column\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the cleaned data for us to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0              enjoying beautiful day park\n",
      "1                 finished amazing workout\n",
      "2         excited upcoming weekend getaway\n",
      "3       feeling grateful little thing life\n",
      "4    rainy day call cozy blanket hot cocoa\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['cleaned_text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Preprocess using Bag of Words vectorization (CountVectoriser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts text into numerical features\n",
    "tfidf = CountVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "X = tfidf.fit_transform(df['cleaned_text'])   # Feature (1st column)\n",
    "y = df['label']                       # Labels  (2nd column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Step 5: Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 75 test 25\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Step 6 : Train the Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'alpha': 0.1, 'class_prior': [0.3, 0.4, 0.3], 'fit_prior': True}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB(alpha=0.1, class_prior=[0.3, 0.4, 0.3])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB(alpha=0.1, class_prior=[0.3, 0.4, 0.3])</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=[0.3, 0.4, 0.3])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()     # best for text classification\n",
    "\n",
    "# Define the parameter grid > For multinomialNB\n",
    "param_grid = {\n",
    "    'class_prior': [None, [0.3, 0.4, 0.3], [0.2, 0.5, 0.3]],\n",
    "    'alpha': [1.0, 0.5, 0.1],\n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "model = MultinomialNB(**best_params)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.85      0.84       112\n",
      "     neutral       0.81      0.78      0.80       111\n",
      "    positive       0.89      0.91      0.90       137\n",
      "\n",
      "    accuracy                           0.85       360\n",
      "   macro avg       0.85      0.85      0.85       360\n",
      "weighted avg       0.85      0.85      0.85       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some sample texts I have written to test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive\n",
    "sample_text = [\"Wow this spa really turned out to real good\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More focused sentiments: Mixed sentiments, sarcasm, genz slang, extreme sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = [\n",
    "    # Positive\n",
    "    \"This coffee shop has the most amazing atmosphere and perfect latte art!\",\n",
    "    \"I'm absolutely loving how productive I've been this week - everything's falling into place!\",\n",
    "    \"The sunset tonight was breathtaking, all pink and gold over the water.\",\n",
    "\n",
    "    # Neutral\n",
    "    \"The meeting has been rescheduled to Thursday at 2pm in Conference Room B.\",\n",
    "    \"My grocery list contains milk, eggs, bread, and vegetables.\",\n",
    "    \"The train to Chicago departs at 3:15pm from Platform 4.\",\n",
    "\n",
    "    # Negative\n",
    "    \"My flight got delayed 6 hours and now I'm stuck sleeping in this awful airport chair.\",\n",
    "    \"The so-called 'fresh' fish I ordered tasted like it was caught last month.\",\n",
    "    \"Nothing puts me in a great mood like getting 50 spam emails before breakfast.\",\n",
    "\n",
    "    # Mixed tone (positive start, negative ending)\n",
    "    \"The concert started great until someone spilled beer all over my new jacket.\",\n",
    "    \"Loved the hotel room, but the construction noise started at 6am sharp.\",\n",
    "\n",
    "    # Long/complex examples\n",
    "    \"After waiting 45 minutes past my appointment time in a crowded waiting room with screaming children, the dentist told me they'd have to reschedule because they 'ran out of time\",\n",
    "    \"The package arrived right on schedule and in perfect condition, with everything exactly as described - a rare and wonderful online shopping experience!\",\n",
    "    \"According to the weather report, there's a 60% chance of rain this afternoon, with temperatures peaking around 72°F before dropping in the evening.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For bunch for samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'This coffee shop has the most amazing atmosphere and perfect latte art!' => Prediction: 'positive'\n",
      "Text: 'I'm absolutely loving how productive I've been this week - everything's falling into place!' => Prediction: 'positive'\n",
      "Text: 'The sunset tonight was breathtaking, all pink and gold over the water.' => Prediction: 'positive'\n",
      "Text: 'The meeting has been rescheduled to Thursday at 2pm in Conference Room B.' => Prediction: 'neutral'\n",
      "Text: 'My grocery list contains milk, eggs, bread, and vegetables.' => Prediction: 'neutral'\n",
      "Text: 'The train to Chicago departs at 3:15pm from Platform 4.' => Prediction: 'neutral'\n",
      "Text: 'My flight got delayed 6 hours and now I'm stuck sleeping in this awful airport chair.' => Prediction: 'negative'\n",
      "Text: 'The so-called 'fresh' fish I ordered tasted like it was caught last month.' => Prediction: 'neutral'\n",
      "Text: 'Nothing puts me in a great mood like getting 50 spam emails before breakfast.' => Prediction: 'positive'\n",
      "Text: 'The concert started great until someone spilled beer all over my new jacket.' => Prediction: 'positive'\n",
      "Text: 'Loved the hotel room, but the construction noise started at 6am sharp.' => Prediction: 'neutral'\n",
      "Text: 'After waiting 45 minutes past my appointment time in a crowded waiting room with screaming children, the dentist told me they'd have to reschedule because they 'ran out of time' => Prediction: 'neutral'\n",
      "Text: 'The package arrived right on schedule and in perfect condition, with everything exactly as described - a rare and wonderful online shopping experience!' => Prediction: 'neutral'\n",
      "Text: 'According to the weather report, there's a 60% chance of rain this afternoon, with temperatures peaking around 72°F before dropping in the evening.' => Prediction: 'positive'\n"
     ]
    }
   ],
   "source": [
    "# Loop through each sentence in sample_texts, transform it, and make predictions\n",
    "for text in sample_texts:\n",
    "    # Transform the single text using the TF-IDF vectorizer\n",
    "    sample_tfidf = tfidf.transform([text])  # use of a list to keep the format\n",
    "\n",
    "    # Make prediction using the trained model\n",
    "    sample_prediction = model.predict(sample_tfidf)\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Text: '{text}' => Prediction: '{sample_prediction[0]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the sample text using same TF-IDF transform function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tfidf = tfidf.transform(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_predictions = model.predict(sample_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'Wow this spa really turned out to real good'\n",
      "Predicted Sentiment: 'positive'\n"
     ]
    }
   ],
   "source": [
    "print(f\"Text: '{sample_text[0]}'\")\n",
    "print(f\"Predicted Sentiment: '{sample_predictions[0]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as /home/sakhaglobal/Documents/GitHub/text_summarisation_trad_NLP/sentiment_model.pkl\n",
      "Vectorizer saved as /home/sakhaglobal/Documents/GitHub/text_summarisation_trad_NLP/vectorizer.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the path where you want to save the model\n",
    "model_path = '/home/GitHub/text_summarisation_trad_NLP/sentiment_model.pkl'\n",
    "\n",
    "# Save the model\n",
    "with open(model_path, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "print(f\"Model saved as {model_path}\")\n",
    "\n",
    "# Saving the vectoruser\n",
    "\n",
    "# Save the fitted vectorizer to a .pkl file\n",
    "vectorizer_path = '/home/GitHub/text_summarisation_trad_NLP/vectorizer.pkl'\n",
    "with open(vectorizer_path, 'wb') as file:\n",
    "    pickle.dump(tfidf, file)\n",
    "\n",
    "print(f\"Vectorizer saved as {vectorizer_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
